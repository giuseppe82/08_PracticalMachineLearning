---
title: "Practical Machine Learning: Quiz week 3"
author: "Giuseppe Di Bernardo"
date: "May 16, 2016"
output: html_document
---
# 1.
Load the cell segmentation data from the AppliedPredictiveModeling package using the commands:

```{r}
# load segmentationOriginal data set
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
```
1. Subset the data to a training set and testing set based on the Case variable in the data set:

```{r}
# create test/train data sets
inTrain = createDataPartition(y=segmentationOriginal$Case, p = 3/4, list = FALSE)
training = segmentationOriginal[inTrain,]
testing  = segmentationOriginal[-inTrain,]
# alternative method
# training = subset(segmentationOriginal, Case == "Train")
# testing  = subset(segmentationOriginal, Case == "Test")
```
2. Set the seed to 125 and fit a CART model with the `rpart` method using all predictor variables and default caret settings.
```{r}
library(rpart) 
set.seed(125)
# fit clssification tree as a model 
modFit <- train(Class ~ ., method = "rpart", data = training)
# print the classification tree
print(modFit$finalModel)
```

3. In the final model what would be the final model prediction for cases with the following variable values:

a. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2

b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100

c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100

d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2

```{r}
library(ggplot2)
suppressMessages(library(rattle))
library(rpart.plot)
rattle::fancyRpartPlot(modFit$finalModel)
```

# 2.
If K is small in a K-fold cross validation is the bias in the estimate of out-of-sample (test set) accuracy smaller or bigger? If K is small is the variance in the estimate of out-of-sample (test set) accuracy smaller or bigger. Is K large or small in leave one out cross validation?

The bias is smaller and the variance is smaller. Under leave one out cross validation K is equal to one. (False)

The bias is smaller and the variance is bigger. Under leave one out cross validation K is equal to one. (False)

The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to the sample size. (True)

The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to two. (False)

# 3.
Load the olive oil data using the commands:
```{r}
library(pgmm)
data(olive)
olive = olive[,-1]
```
These data contain information on 572 different Italian olive oils from multiple regions in Italy. Fit a classification tree where Area is the outcome variable. 
```{r}
# fit classification tree as a model
modFit <- train(Area ~ ., method = "rpart", data = olive)
# print the classification tree
print(modFit$finalModel)
# plot the classification tree
rattle::fancyRpartPlot(modFit$finalModel)
```

Then predict the value of area for the following data frame using the tree command with all defaults

```{r}
newdata = as.data.frame(t(colMeans(olive)))
# predict on newdata values
predict(modFit, newdata = newdata)
```

# 4. 
Load the South Africa Heart Disease Data and create training and test sets with the following code:
```{r}
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
```

Then set the seed to 13234 and fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors. 
```{r}
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
```

Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
```{r}
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
prediction_trainset <- predict(modFit, trainSA)
prediction_testset <- predict(modFit, testSA)
missClass(trainSA$chd,prediction_trainset)
missClass(testSA$chd,prediction_testset)
```

# 5. 
Load the vowel.train and vowel.test data sets:
```{r}
# load data
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
```

Set the variable y to be a factor variable in both the training and test set. Then set the seed to 33833.
```{r}
# create train/test data sets
training <-vowel.train
testing <- vowel.test
set.seed(33833)
```

Fit a random forest predictor relating the factor variable y to the remaining variables. Read about variable importance in random forests here: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr The caret package uses by default the Gini importance.

```{r}
# apply random forest
# modFit <- train (as.factor(y) ~ ., data = training, method = "rf")
# apparently, it gets a different result. For the quiz, use randomForest() function
# head(getTree(modFit$finalModel, k = 2))
library(randomForest)
modFit <- randomForest(as.factor(y) ~., data = training)
# return the second tree 
order(varImp(modFit), decreasing = T)
```